name: llm
services:
  open-webui:
    container_name: open-webui
    image: ghcr.io/open-webui/open-webui:main
    restart: always
    environment:
      - TZ=${TZ}
      - "OLLAMA_BASE_URL=http://ollama:11434"
    volumes:
      - ${APP_DATA}/llm/open-webui:/app/backend/data
    depends_on:
      - ollama
      - litellm
    labels:
      - traefik.enable=true
      - traefik.http.routers.openwebui.rule=Host(`chat.${ROOT_DOMAIN}`)
      - traefik.http.routers.openwebui.tls=true
      - traefik.http.routers.openwebui.entrypoints=websecure
      - traefik.http.routers.openwebui.tls.certresolver=resolver
      - traefik.http.services.openwebui.loadbalancer.server.port=8080

  ollama:
    container_name: ollama
    image: ollama/ollama
    restart: always
    hostname: ollama
    environment:
      - TZ=${TZ}
      - OLLAMA_KEEP_ALIVE=24h
    volumes:
      - ${APP_DATA}/llm/ollama:/root/.ollama
    devices:
      - nvidia.com/gpu=all
    labels:
      - traefik.enable=true
      - traefik.http.routers.ollama.rule=Host(`ollama.${ROOT_DOMAIN}`)
      - traefik.http.routers.ollama.tls=true
      - traefik.http.routers.ollama.entrypoints=websecure
      - traefik.http.routers.ollama.tls.certresolver=resolver
      - traefik.http.services.ollama.loadbalancer.server.port=11434

  litellm:
    container_name: litellm
    restart: always
    image: ghcr.io/berriai/litellm:main-latest
    env_file: ".env"
    # ports:
    #   - "4000:4000"
    volumes:
      - ${APP_CONFIG}/llm/litellm/config:/config
    command:
      - --config=/config/config.yaml
    labels:
      - traefik.enable=true
      - traefik.http.routers.litellm.rule=Host(`litellm.${ROOT_DOMAIN}`)
      - traefik.http.routers.litellm.tls=true
      - traefik.http.routers.litellm.entrypoints=websecure
      - traefik.http.routers.litellm.tls.certresolver=resolver
      - traefik.http.services.litellm.loadbalancer.server.port=4000
